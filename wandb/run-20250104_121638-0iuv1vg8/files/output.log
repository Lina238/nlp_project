
Utilisation du device: cpu

Tentative de chargement du modèle depuis retrain_epoch_1...
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Modèle chargé avec succès depuis le fichier sauvegardé

Loading dataset...

Taille du dataset d'entraînement: 9000
Taille du dataset de validation: 1000

Initializing MedicalQADataset with max_length=512
Dataset initialized with 9000 examples

Initializing MedicalQADataset with max_length=512
Dataset initialized with 1000 examples

Starting retraining process with 3 epochs...

Epoch 1/3
C:\Python312\Lib\site-packages\transformers\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(

Epoch 1/1
Epoch 1/1:   0%|                                                      | 1/1125 [01:21<25:25:57, 81.46s/it, loss=5.5003]Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors
Epoch 1/1:  13%|██████▋                                            | 147/1125 [1:28:21<9:47:54, 36.07s/it, loss=0.7046]
Traceback (most recent call last):
  File "C:\Users\Lina-pc\OneDrive\Bureau\nlp_devoir_q&a\retrain.py", line 154, in <module>
    retrain_model("retrain_epoch_1", num_epochs=3, sample_size=10000)
  File "C:\Users\Lina-pc\OneDrive\Bureau\nlp_devoir_q&a\retrain.py", line 136, in retrain_model
    train_model(
  File "C:\Users\Lina-pc\OneDrive\Bureau\nlp_devoir_q&a\retrain.py", line 50, in train_model
    loss.backward()
  File "C:\Python312\Lib\site-packages\torch\_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "C:\Python312\Lib\site-packages\torch\autograd\__init__.py", line 347, in backward
    _engine_run_backward(
  File "C:\Python312\Lib\site-packages\torch\autograd\graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
